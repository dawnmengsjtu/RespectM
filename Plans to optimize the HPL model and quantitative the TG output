#Plans to optimize the HPL model and quantitative the TG output

#Plan for brute-force optimization of model
We plan to use brute-force algorithms to exhaustively enumerate all combinations of inputs and outputs so that each value can be trained, therefore, optimizing the HPL model, and contributing to finding the global optimal solution.
The brute-force search algorithm enumerates all possible feature combinations in the dataset, ensuring it can find the optimal subset of features. Specifically, assuming there are “d” features in the dataset, each feature has two states (i.e., taken or not taken), and the total number of possible feature combinations is 2d. Therefore, the brute-force search algorithm can enumerate all possible feature combinations and select the subset with the best performance as the final result.
We can first read the input and output features and obtained the column names, then constructed all possible feature combinations among input and output. After that, we built a model using each feature combination and evaluated the model performance on the test set. We compared the models' performance and kept the best model and performance metric. Finally, we used the best model to predict all samples and calculate the model performance metrics.
However, the complexity of the brute-force algorithm increases exponentially with the problem size and the number of feature dimensions when searching for all possible solutions. This can require an enormous amount of computational resources. If we were to perform a brute-force search to optimize the model on a higher-dimensional dataset, it is estimated that it would take more than two months under our current conditions.

#Plan for convex optimization
We plan to use convex optimization methods to optimize the HPL model, and accurately find the global optimum of two-layer ReLU neural networks according to the previous reports and quantitative the output.
There are currently proposed solutions in theory for the global optimum solution of neural networks. We provided two related references as follows:
[1] Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial- time convex optimization formulations for two-layer networks. In International Conference on Machine Learning, pages 7695–7705. PMLR, 2020.
[2] Yifei Wang, Jonathan Lacotte, and Mert Pilanci. The hidden convex optimization landscape of regularized two-layer relu networks: an exact characterization of optimal solutions. In International Conference on Learning Representations, 2021.
They both discuss finding the global optimum in two-layer ReLU neural networks. In the first paper, the authors proposed an exact and polynomial-time convex optimization formulation for finding the global optimum of two-layer ReLU neural networks. They represented the neural network parameters as a convex optimization problem with non-linear constraints and then transformed it into a problem of finding the minimum value on a convex set. In this problem, the constraints come from the non-linearity of the ReLU activation function, and the objective comes from the loss function. The authors proved that the problem is convex and provided a polynomial-time algorithm to solve it.
The authors further explored the optimization problem in two-layer ReLU neural networks in the second paper. They studied the hidden convex optimization landscape of the network parameters and proposed an exact optimal solution characterization method. They found that the optimization problem was convex and provided an accurate method for computing the global optimum. Furthermore, they demonstrated the effectiveness of this method in practical applications.
The two papers use convex optimization methods to accurately solve the global optimum of two-layer ReLU neural networks and explore the problem's hidden convex optimization landscape of the problem.
Finally, we can use a QP solver such as CVXPY or Gurobi to solve the optimization problem defined above and obtain the weight and bias parameters of the two-layer neural network. If the solver finds a solution, it is the global optimal solution for the problem.
Based on the efforts above, we can find or approximate the global optimum based on the optimized HPl model. To further quantitative the TG output, we can combine the HPL model with other quantitative methods such as metabolic flux analysis or metabolic kinetic analysis can help researchers to determine the quantitative effects of metabolite changes, and optimize the efficiency and stability of the modeling process. Overall, these efforts can lead to a better understanding of the complex relationships between TG and other metabolites and provide insights into optimizing the production of TG.
